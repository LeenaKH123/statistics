---
title: "OSTA5003 Week 3 Homework"
format:
    html:
        embed-resources: true
---

# Wine quality reviews

Load the wine quality data and remove the id column

```{r}
# load necessary library
suppressPackageStartupMessages(library(tidyverse))
# Load the data
wine_dat <- read.csv("Project3/winequality-data.csv", stringsAsFactors = TRUE)
# Remove he id column
wine_dat[["id"]] <- NULL
# view the first few rows
head(wine_dat)
```

## Create training and test split

Split the dataset into 50% training and 50% testing.

```{r load}
#| eval: false
# Delete the eval line (or set to true) once you fix the code below
# Use caret to create a training and test split
library(caret)
set.seed(5003)
indices <- #
train_dat <- #
test_dat <- #
```

## Calibrating Random Forest

Assess the performance of a random forest model using `ranger`. Build the model on the training data and assess against the test data. In particular, consider how the performance changes as you vary the number of trees in the forest.

* Create a vector of values for the number of trees to try. You should try at least 10 different values of the number of trees and include the case of 1 tree (almost a decision tree).
* For each value of the number of trees, build a random forest model using `ranger` and assess the performance on the test data. Use the accuracy on the test data as the performance metric.
* Visualize the results plotting the number of trees against the accuracy on the test data.

```{r split}
#| eval: false
# Delete the eval line (or set to true) once you fix the code below
ntree_seq <- #
```

Using the `ntree_seq` vector, do the three steps mentioned above in two ways.

### Approach A

Fit a random forest model for each specified number of trees and store the accuracy on the test data in a vector and visualize the results.

```{r rfA}

```

### Approach B

Modify the second step by only fitting one single random forest using the largest number of trees in the sequence. Resuse the same random forest but only predict the accuracy on the test set using the number of trees specified in the vector `ntree_seq`.

```{r rfB}

```

# Construct a 10-fold cross validation

Design a 10-fold cross-validation procedure to evaluate a kNN classification model (with $k=5$) accuracy. Use `class::knn()` and NOT the `caret` package (but you can use the caret package to create validation cross fold partitions.)

## Create a 10 fold split

Create a 10-fold split and partition up the data into the appropriate training, test splits to be used in the `knn` call

```{r kfolds}
#| eval: false
# Delete the eval line above (or set to true) once you fix the code below
kfolds <- createFolds(wine_dat$quality, k = 10)
# Function that takes observation indices, creates training and test split
# returns a list with two data.frames (training and test)
create_training_and_test <- function(index, data, k = 10) {
  train_data <- data[, ] # incomplete code, use index in the right way
  test_data <- data[, ]
  list(training = train_data, test = test_data)
}
kfold_training_and_test <- lapply(kfolds, create_training_and_test)
```

## Compute the predicted values on each test fold

Using your training test split over the 10 folds. Fit the `knn` models and extract the observed and predicted class labels for each of the 10 folds. (_Hint_: Write a function that does this prediction step for later use, ideally it should have arguments for the training and test data and perhaps the number of nearest neighbours)

```{r fit}
#| eval: false
# Delete the eval line (or set to true) once you fix the code below
library(class)
fit_knn_and_reports_obs_and_pred <- function(data_list, k = 5) {
  train_data <- data_list[["training"]]
  test_data <- data_list[["test"]]
  train_arg <- train_data[] # Remove the target/outcome column
  test_arg <- test_data[] # Remove the target/outcome column
  cl_arg <- train_data[] # Extract the target/outcome
  model <- knn(
    train = train_arg,
# More code here
    k = k)
  list(observed = .., predicted = ...)
}
obs_and_pred <- lapply(kfold_training_and_test, fit_knn_and_reports_obs_and_pred)
```

## Compute the performance metrics

Using the predicted and observed values in each case, calculate the sensitivity, specificity, accuracy and $F_1$ score for the kNN classifier. You may use the `caret::confusionMatrix` or an equivalent helper function from another package if you wish. Look at the performance and identify if it has good/bad performance in the metrics.

```{r performance}
#| eval: false
performance_calcs <- function(obs_and_pred) {
  # Code that computes accuracy and confusion matrix
}
lapply(obs_and_pred, performance_calcs)
```

# Extend to write your own repeated cross validation

Use your code above to conduct a repeated cross validation of say `m = 100` runs (or however many you wish given the speed of your code or computation power of your hardware) to construct a sample of CV performance estimates for Accuracy, Sensitivity, Specificity and the F1 scores. Visualize your estimates. You may use the `caret::createMultiFolds` or create your own if you wish. The `createMultiFolds` function returns the data in a slightly different format (see the documentation at `? createFolds`)

```{r}

```
