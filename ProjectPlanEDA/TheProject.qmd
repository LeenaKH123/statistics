---
title: "The Project"
format:
  html:
    code-fold: true
    code-tools: true
    toc: true
    theme: flatly
---

Data Source
https://www.kaggle.com/code/sudhanshu2198/multi-class-credit-score-classification/input?select=Score.csv

Introduction:
This report presents the exploratory data analysis (EDA) for a credit score classification problem. The goal is to understand the structure, qualiy and statistical properties of the dataset to inform feature selection and model development in subsequent phases.

1. Data Loading and Initial Inspection
The training, test and the benchmark scoring datasets are imported and examined to uderstand their structure and and valriable types.

```{r}
# Load required libraries
library(tidyverse)
library(janitor)
library(skimr)
library(DataExplorer)
library(corrplot)
library(Amelia)

# Load the data
train <- read.csv("train.csv", stringsAsFactors = TRUE)
test <- read.csv("test.csv", stringsAsFactors = TRUE)
score <- read.csv("Score.csv", stringsAsFactors = TRUE)

# View dimensions
dim(train)
dim(test)
dim(score)

# Quick glimpse
glimpse(train)
```

2. Assessment of missing values
A through check for missing values is conducted to determine the extent and distribution of incompleteness across the dataset
```{r}
colSums(is.na(train))
skim(train)
```

3. Visualise missing values
Yellow represents missing values, and black represents available values. Blocks of missingness in specific columns indicate a need for either targeted imputation or column removal, especially if missingness is extensive or not missing at random (NMAR).
```{r}
library(Amelia)  # For missmap visualization
missmap(train, 
        main = "Missing Values Map", 
        col = c("yellow", "black"), 
        legend = TRUE)      
```
only about 1% of the dataset is missing, which is manageable, the dataset is mostly complete. Missing data is not concentrated in a few variables it is spread across multiple features. Columns like Monthly_Balance, Amount_invested_monthly and Changed_Credit_Limit appear to have more missing entries that others. This suggests random missingness.


4. Data type correction
Several columns originally read as character variables contain numeric data. These are converted to numeric or categorical types to enable statistical analysis.
```{r}
# Define a function to clean and convert character values to numeric
clean_numeric <- function(x) {
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("[$%]", "", x)
  x <- gsub("[^0-9\\.\\-]", "", x)
  suppressWarnings(as.numeric(x))
}

# List of columns expected to be numeric but possibly read as character
cols_to_convert <- c("Age", "Annual_Income", "Outstanding_Debt", "Num_of_Loan", 
                     "Changed_Credit_Limit", "Num_of_Delayed_Payment", 
                     "Amount_invested_monthly", "Monthly_Balance")

# Check which columns exist in the data before applying
cols_to_convert <- intersect(cols_to_convert, colnames(train))

# Apply cleaning function only to existing columns
for (col in cols_to_convert) {
  train[[col]] <- clean_numeric(train[[col]])
}

# Convert these columns to factors only if they exist
if ("Credit_Mix" %in% names(train)) {
  train$Credit_Mix <- as.factor(train$Credit_Mix)
}
if ("Payment_of_Min_Amount" %in% names(train)) {
  train$Payment_of_Min_Amount <- as.factor(train$Payment_of_Min_Amount)
}

# Show summary of the cleaned dataset
summary(train)

 
```
The dataset has been successfully loaded and cleaned, with several formerly charecter-based numeric fields now recognised as numeric, there are some outliers and invalid entries and multiple columns with missing values that require further cleaning and imputation.
Age: contains invalid ages (eg: -500 and 8698)
Annual_Income: extreme outliers likely caused by errors or data entry issues.
Monthly_Inhand_Salary: missing in ~15% of rows.
Num_Bank_Accounts, Num_Credit_Card, Interest_Rate: these values suggest data errors.
Num_of_Loan: -ve loan count is invalid
Credit_History_Age: missing values in over 9,000 rows.
Payment_of_Min_Amount: correctly converted to factor, this column is clean.
Amount_invested_monthly and Monthly_Balance: low missingness can be imputed safely.
Categorical Noise: some columns contain nonsensical strings, such as SSN: Values like #F%$D@*&8, Payment_Behaviour: Value like !@9#%8, Occupation: Many are listed as "_______".
Target Variable - Credit_Score: consistent with earlier findings, class imbalance should be addressed in modeling.


6. Target Variable Check - Credit Score
Examine the distribution of the target variable Credit_Score that is to be predicted. To understand if the datset is balanced or imbalanced across the different credit score categories. If one class dominates the others, the model might favor that class.
```{r}
table(train$Credit_Score)
prop.table(table(train$Credit_Score)) * 100
              
```
This tells us that 53.2% of the records are standard, 29% are poor, 17.8% are good. The dataset is imbalanced, the standard class is overrepresnted, a classification model trained on this data might favor the standard class unless balancing techniques are applied such as: SMOTE, class weights, resampling. Or evaluate with metrics beyong accuracy such as F1-score, precision/recall per class.






5. Removal of Irrelevant Features
Columns such as identifiers and personal details (Ex: ID, Name, SSN) are removed as they do not contribute predictive value to the modeling task.
```{r}
    colnames(train)

```

6. PCA for visualisation
```{r}

              
```

7. Distribution Plot
```{r}
       
```

```{r}
               
```

```{r}
              
```

```{r}
               
```
```{r}
               # Dimensions (rows, columns)
```

```{r}
               # Dimensions (rows, columns)
```

```{r collapse=TRUE}
               # Dimensions (rows, columns)
```

```{r collapse=TRUE}
               # Dimensions (rows, columns)
```