---
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: flatly
    fig-align: center
    self-contained: true
    css: style.css
---

```{r, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 4,
  fig.height = 3,
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)
```
# Credit Score Classification – EDA & Project Plan
# Overview of the problem
The goal of this project is to build a multi-class classification model to predict a customer’s Credit Score (Poor, Standard, Good) using demographic and financial behavior data. This can support automated risk assessment in financial institutions and reduce manual workload.

## Why this problem is interesting
- Real-world impact: Credit score classification plays a crucial role in lending decisions, insurance pricing, and financial eligibility assessments.
- Data complexity: The dataset presents real-life challenges including:
- Missing values
- Noisy and inconsistent entries (e.g. placeholders like “_______”, symbols like #F%$D)
- Class imbalance
- Outliers and corrupted numeric fields
- Machine learning relevance: Offers a rich opportunity to apply supervised classification techniques, handle imbalanced classes, and test advanced models like Random Forest and XGBoost.

# Dataset Overview
- Source:	Kaggle (https://www.kaggle.com/code/sudhanshu2198/multi-class-credit-score-classification)
- train.csv: Used for EDA, data cleaning, and model training. Contains features and the target label Credit_Score.
- test.csv: Used to evaluate model performance on unseen data. Contains features only.
- score.csv: Used for final predictions and submission. Same structure as the test set.
- Samples	~100,000
- Features:	Mix of numeric and categorical
- Target:	Credit_Score (Poor, Standard, Good)
- Key Issues:	Outliers, noisy strings, class imbalance
- Missing Values	~1% overall; highest in Credit_History_Age, Monthly_Inhand_Salary

```{r, include=FALSE}
library(tidyverse)
library(janitor)
library(skimr)
library(Amelia)
library(corrplot)
library(factoextra)
library(tidyr)
train <- read.csv("train.csv")
```

# Missing Value Assessment

```{r}
colSums(is.na(train))
missmap(train, main = "Missing Data Map", col = c("yellow", "black"))
```
**Summary:** ~1% missing. Most common: `Credit_History_Age`, `Monthly_Inhand_Salary`, `Amount_invested_monthly`

# Data Cleaning and Type Fixes

```{r}
clean_numeric <- function(x) {
  x <- gsub(",", "", as.character(x))
  x <- gsub("[$%]", "", x)
  x <- gsub("[^0-9\\\\.\\\\-]", "", x)
  suppressWarnings(as.numeric(x))
}

cols_to_clean <- c("Age", "Annual_Income", "Outstanding_Debt", "Num_of_Loan", 
                   "Changed_Credit_Limit", "Num_of_Delayed_Payment", 
                   "Amount_invested_monthly", "Monthly_Balance")

for (col in intersect(cols_to_clean, names(train))) {
  train[[col]] <- clean_numeric(train[[col]])
}

train$Credit_Mix <- as.factor(train$Credit_Mix)
train$Payment_of_Min_Amount <- as.factor(train$Payment_of_Min_Amount)
```
- Age: contains invalid ages (eg: -500 and 8698)
- Annual_Income: extreme outliers likely caused by errors or data entry issues.
- Monthly_Inhand_Salary: missing in ~15% of rows.
- Num_Bank_Accounts, Num_Credit_Card, Interest_Rate: these values suggest data errors.
- Num_of_Loan: -ve loan count is invalid
- Credit_History_Age: missing values in over 9,000 rows.
- Payment_of_Min_Amount: correctly converted to factor, this column is clean.
- Amount_invested_monthly and Monthly_Balance: low missingness can be imputed safely.
- Categorical Noise: some columns contain nonsensical strings, such as SSN: Values like #F%$D@*&8, Payment_Behaviour: Value like !@9#%8, Occupation: Many are listed as "_______".
- Target Variable - Credit_Score: class imbalance should be addressed in modeling.
# Target Class Distribution

```{r}
ggplot(train, aes(x = Credit_Score, fill = Credit_Score)) +
  geom_bar() +
  labs(title = "Credit Score Distribution", y = "Count") +
  theme_minimal()
```
The dataset has class imbalance, with the "Standard" credit score category being the most prevalent, followed by "Poor" and then "Good", which may bias classification models and necessitates the use of resampling or class-weighting techniques.


# Outlier Detection

```{r}
ggplot(train, aes(x = "", y = Age)) +
  geom_boxplot(fill = "orange") +
  labs(title = "Age Boxplot") +
  theme_minimal()
```
The boxplot of the Age variable reveals the presence of extreme outliers, with some values exceeding 7500, which are clearly unrealistic and indicate data entry errors. These anomalies should be investigated and removed or corrected to ensure accurate analysis and model training.

# Correlation and PCA

## Correlation Matrix (Simplified for Clarity)

```{r, fig.width=5, fig.height=4}
library(corrplot)

# Select only numeric columns and drop NAs
numeric_data <- train %>%
  select(where(is.numeric)) %>%
  drop_na()

# Simple and clean correlation heatmap
corrplot(
  cor(numeric_data),
  method = "color",
  type = "upper",           # Upper triangle only
  tl.cex = 0.6,             # Label font size
  tl.col = "black",         # Label color
  mar = c(0, 0, 2, 0),      # Margin to fit the title
  title = "Correlation Matrix of Numeric Features"
)

```
- Are there features that are highly correlated?
- Strong Positive Correlations:
- Monthly_Inhand_Salary & Monthly_Balance: ≈ 0.70
- Monthly_Inhand_Salary & Amount_invested_monthly: ≈ 0.63
These two pairs show strong positive correlation, indicating that: Higher income tends to result in higher savings and investment.
- Moderate Negative Correlations:
- Outstanding_Debt shows moderate negative correlation with:
- Monthly_Inhand_Salary
- Monthly_Balance
- This suggests that individuals with higher debt may have less monthly cash available.
- Minimal Correlations:
- Features like: Num_Credit_Card, Num_Bank_Accounts, Interest_Rate
Show weak or no correlation with other variables — which is good for model diversity and reducing feature redundancy.
- Only a couple of features are highly correlated. Most features contribute independent information, which is beneficial for machine learning models and avoids multicollinearity issues.

## PCA Plot

```{r, fig.width=4, fig.height=3}
pca_data <- train %>%
  select(where(is.numeric)) %>%
  na.omit()

credit_labels <- train$Credit_Score[as.numeric(rownames(pca_data))]

pca_scaled <- scale(pca_data)
pca_result <- prcomp(pca_scaled, center = TRUE, scale. = TRUE)

fviz_pca_ind(pca_result,
             geom.ind = "point",
             col.ind = as.factor(credit_labels),
             legend.title = "Credit Score") +
  labs(title = "PCA - Credit Score Classes")
```
The PCA plot shows the distribution of credit score classes along the first two principal components, which together explain 18.5% of the total variance in the dataset. While there is some clustering, particularly for the "Standard" class, the "Good" and "Poor" classes show significant overlap. This indicates that the classes are not linearly separable in reduced dimensions, suggesting that more complex modeling techniques may be required to effectively distinguish between credit score categories.

# Evaluation Metrics & Algorithms
The classification model will be evaluated using several key metrics suited for multi-class problems:<br>
- Accuracy: Measures the overall correctness of predictions.<br>
- Precision, Recall, and F1-Score (calculated per class): Help assess how well the model distinguishes between Poor, Standard, and Good credit scores.<br>
- Macro and Weighted F1-Scores: Macro F1 treats all classes equally, while Weighted F1 adjusts for class imbalance by accounting for the frequency of each class.<br>
Confusion Matrix: Offers a detailed breakdown of correct and incorrect classifications to identify potential model bias or confusion between classes.<br>
Model performance will be validated using k-fold cross-validation to ensure generalizability. Metrics will be computed on both training and test sets, with emphasis on F1 performance.<br>
The following algorithms will be implemented and compared:<br>
- Logistic Regression: A baseline linear model for multi-class classification.<br>
- Decision Tree: A simple and interpretable non-linear model.<br>
- Random Forest: An ensemble approach that reduces overfitting through averaging.<br>
- XGBoost: A gradient boosting framework optimized for speed and performance, particularly in structured data tasks.<br>

# Project Plan

## Phase Plan (3 Phases)
- **Phase 1**: Data Cleaning & Exploration  
- **Phase 2**: Modeling & Evaluation (Logistic, Tree, RF, XGBoost)  
- **Phase 3**: Prediction & Reporting

## Data Usage
- **`train.csv`** will be used for data cleaning, EDA, model training, and validation. It contains both features and the target label (`Credit_Score`).
- **`test.csv`** will be used to evaluate the trained model on unseen data. It contains input features but no labels.
- **`score.csv`** will be used for final prediction and model deployment simulation. It is structurally similar to the test set and is used to demonstrate how the model performs in real-world application.

## Weekly Timeline with Subtasks

| Week     | Tasks |
|----------|-------|
| **Week 1** | - Load and clean `train.csv` <br> - Remove invalid and placeholder values <br> - Impute missing data <br> - Convert types and encode categorical variables <br> - Perform EDA (class imbalance, outliers, correlation, PCA) |
| **Week 2** | - Train models using `train.csv` (Logistic Regression, Decision Tree, Random Forest, XGBoost) <br> - Perform cross-validation <br> - Evaluate using Accuracy, Precision, Recall, Macro/Weighted F1 <br> - Generate and interpret confusion matrix |
| **Week 3** | - Apply the best model to `test.csv` and `score.csv` <br> - Export predictions for `score.csv` <br> - Finalize report with evaluation results <br> - Present findings |
<br>
<br>

```{r, fig.width=10, fig.height=6, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(lubridate)

# Gantt chart data
gantt_data <- tribble(
  ~Task, ~Start, ~End, ~Phase,
  "Load and clean train.csv", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Remove invalid & placeholder values", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Impute missing data", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Encode categorical variables", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "EDA: imbalance, outliers, correlation, PCA", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  
  "Train models using train.csv", "2025-05-25", "2025-05-31", "Modeling & Evaluation",
  "Cross-validation & tuning", "2025-05-25", "2025-05-31", "Modeling & Evaluation",
  "Evaluate: Accuracy, F1, Confusion Matrix", "2025-05-25", "2025-05-31", "Modeling & Evaluation",

  "Apply best model to test.csv & score.csv", "2025-06-01", "2025-06-03", "Prediction & Reporting",
  "Export predictions for score.csv", "2025-06-01", "2025-06-03", "Prediction & Reporting",
  "Finalize report with evaluation results", "2025-06-01", "2025-06-03", "Prediction & Reporting",
  "Present findings", "2025-06-01", "2025-06-03", "Prediction & Reporting"
) %>%
  mutate(across(c(Start, End), ymd))

# Plot the Gantt chart
ggplot(gantt_data, aes(x = Start, xend = End, y = fct_rev(Task), yend = fct_rev(Task), color = Phase)) +
  geom_segment(size = 6) +
  labs(
    title = "3-Week Project Gantt Chart with Dataset Usage",
    x = "Date", y = NULL, color = "Project Phase"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    legend.position = "bottom",
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9),
    axis.text.y = element_text(size = 9),
    panel.grid.minor = element_blank()
  )
```
