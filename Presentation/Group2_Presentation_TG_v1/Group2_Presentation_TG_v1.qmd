---
format:
  revealjs:
    auto-slide: 3000  # 3 seconds for just this file
    slide-number: false
    controls: false
    progress: false
    smaller: true 
---
## Predicting Credit Scores {.title-slide}
**A Machine Learning Approach for Smarter Lending Decisions**
Group 2
---
format:
  revealjs:
    auto-slide: 20000  # 20 seconds for the rest
    slide-number: true
    controls: true
    progress: true
    smaller: true 
---
## Project Introduction 

- This project aims to build a multi-label classification model to predict the credit score with different categories such as Poor, Standard, Good. 

- Foundation of this project is a source data set with 100K+ observations and having different financial features associated with it.
- Why? To automate risk assessment, reduce manual reviews, and support financial decisions.

## Project Outcomes 

- Some of the outcomes expected out of this project are    
  - Highly scalable, robust and performant classifier to predict Credit Score categories and easily adaptable in BFSI sector
  - Helps in customer segmentation based on Credit profiles
  - Supports automated risk assessment and reduce manual load
  - Empowers Fin sector to proactively address the lending risks and deliver the secured and unsecured credit with high confidence

## Problem Statement 

- Why it's an interesting problem to solve

  - Firstly, achieving the expected outcomes is equally rewarding and challenging
  - Multi-class types ; Huge volume of observations (~100K) and mix of numeric and categorical features
  - Classes are highly imbalanced i.e. majority and minority classes
  - Data Quality issues in terms of missingness, outliers, noise in features etc.
  - Finally, real potential is huge i.e. Credit score classification plays a crucial role in lending decisions, insurance pricing, and financial eligibility assessments.
  - Real-world impact on loans, insurance, and creditworthiness.

## Dataset Overview
 - 100,000+ customer records
 - Features: Income, Age, Debt, Bank Accounts, Occupation, Credit History
 - Target variable: Credit_Score (Poor, Standard, Good)
 - Files: train.csv, test.csv, score.csv

## Data Challenges
 - Invalid ages: -500, 8698
 - Corrupt SSNs: '#F%$D@' 
 - Missing values in (Credit History & Monthly Salary).
 - Target class imbalance: “Standard” dominates (Standard >> Poor/Good) 

## EDA - Visualization (Correlation Matrix) 

```{r correlation matrix}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(corrplot))

# Load the data
score_dat <- read.csv("Score.csv", stringsAsFactors = TRUE)

# Move Target variable to last position
score_dat <- score_dat %>% select(-Credit_Score, Credit_Score)

# Creating matrix
cor_matrix_cc <- cor(score_dat[, sapply(score_dat, is.numeric)], use = "complete.obs")

# Convert to DF for ggplot
cor_data_cc <- melt(cor_matrix_cc)

ggplot(cor_data_cc, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), color = "black", size = 2) +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  theme_minimal() +
  labs(title = "Correlation Heatmap Matrix for Credit Score Dataset ", fill = "Correlation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## PCA: Visualising Credit Profiles
- PCA reduces feature complexity to 2 components  
- Shows separation between customer types  
- Distinct clusters for “Good” vs. others
```{r PCA}
# Load required libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(corrplot))

# Load the data
score_dat <- read.csv("Score.csv", stringsAsFactors = TRUE)

# Check if 'Credit_Score' exists
if (!"Credit_Score" %in% colnames(score_dat)) {
  stop("The column 'Credit_Score' was not found in the dataset.")
}

# Remove rows with missing values (to avoid PCA errors)
score_dat <- na.omit(score_dat)

# Save the target labels
credit_labels <- score_dat$Credit_Score

# Select only numeric features (excluding the target)
numeric.dat <- score_dat %>%
  select(where(is.numeric))  # select all numeric columns

# OPTIONAL: Drop 'Credit_Score' if it's numeric (just to be safe)
if ("Credit_Score" %in% colnames(numeric.dat)) {
  numeric.dat <- numeric.dat %>% select(-Credit_Score)
}

# Run PCA
pca_result <- prcomp(numeric.dat, scale. = TRUE)
pca_df <- as.data.frame(pca_result$x)
pca_df$Credit_Score <- credit_labels  # Add labels back

# Plot PCA
ggplot(pca_df, aes(x = PC1, y = PC2, color = Credit_Score)) +
  geom_point(size = 1.5, alpha = 0.7) +
  labs(title = "PCA: 2D Projection of Customers by Credit Score",
       x = "Principal Component 1",
       y = "Principal Component 2",
       color = "Credit Score") +
  theme_minimal()
```

## Project Phases & Dataset Use

**Goal**: Predict credit scores (Poor, Standard, Good) using ML

**Phase 1: Data Cleaning & Exploration** <br>
- Clean and preprocess `train.csv`<br>
- Handle missing values and invalid entries <br>
- Encode categorical variables <br>
- Conduct EDA (imbalance, correlation, PCA) <br>

**Phase 2: Modeling & Evaluation**
- Train models: Logistic, Decision Tree, Random Forest, XGBoost <br>
- Cross-validation + F1 Score & ROC-AUC evaluation <br>

**Phase 3: Prediction & Reporting**
- Predict on `test.csv` and `score.csv` (real-world simulation) <br>
- Export results <br>
- Finalize report <br>

## Gant Chart

```{r GantChart}
library(tidyverse)
library(lubridate)
library(forcats)
library(ggplot2)

# Define task data
gantt_data <- tribble(
  ~Task, ~Start, ~End, ~Phase,
  "Load and clean train.csv", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Remove invalid & placeholder values", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Impute missing data", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Encode categorical variables", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "EDA: imbalance, outliers, correlation, PCA", "2025-05-18", "2025-05-24", "Data Cleaning & Exploration",
  "Train models (Logistic, Tree, RF, XGBoost)", "2025-05-25", "2025-05-31", "Modeling & Evaluation",
  "Cross-validation & F1 scoring", "2025-05-25", "2025-05-31", "Modeling & Evaluation",
  "Evaluate on test.csv", "2025-06-01", "2025-06-03", "Prediction & Reporting",
  "Export predictions", "2025-06-01", "2025-06-03", "Prediction & Reporting",
  "Finalize report with evaluation results", "2025-06-01", "2025-06-03", "Prediction & Reporting",
  "Present findings", "2025-06-01", "2025-06-03", "Prediction & Reporting"
) %>%
  mutate(across(c(Start, End), ymd))

# Define milestone data
milestones <- tibble(
  Task = "Present key data insights",
  Date = ymd("2025-05-27"),
  Phase = "Milestone"
)

# Reorder factor levels for consistent y-axis
all_tasks <- fct_rev(factor(c(gantt_data$Task, milestones$Task)))

# Plot Gantt chart with milestone
ggplot() +
  geom_segment(
    data = gantt_data,
    aes(x = Start, xend = End, y = Task, yend = Task, color = Phase),
    size = 6
  ) +
  geom_point(
    data = milestones,
    aes(x = Date, y = Task, color = Phase),
    shape = 18, size = 5, stroke = 2
  ) +
  scale_y_discrete(limits = levels(all_tasks)) +
  labs(
    title = "Project Timeline",
    x = "Date", y = NULL, color = "Project Phase"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  )
```


## Known Issue - "Class Imbalanced" 

Class Imbalanced means count of observations belong to classes vary in huge difference.In general, ML models work best with balanced classes, however same cannot be expected from most of the real-world data sets where classes are imbalanced and special ways to treat that issue.

Challenges with Imbalanced Data

- Bias Toward Majority Class and often, ignoring minority patterns.
- Poor Recall for Minority Class ; Fails to detect rare events (e.g., fraud detection or credit classification).

## Ways to handle "Class Imbalanced" issue 

Methods used to solve Class Imbalanced are (applicable to both linear and tree types of ML models)

- Adjusting Class Weights -> Penalizes misclassification of the minority class more
- Oversampling (e.g. SMOTE) or Undersampling -> Balances the dataset before training
- Threshold Optimization -> Instead of default 0.5, choose a better decision threshold
- Adds regularization for better decision boundaries

## Linear Models (Logistic Regression) 

- Logistic Regression
    - Predicts the outcomes that belong to two or more categories.
    - Looks at past data --> Identifies patterns --> Estimates Probability --> Makes Decision
    - Examples
        - High Income -> Chances of "Good" Credit Score
        - High Debt   -> Chances of "Poor" Credit Score
    - Limitations are
        - Majority Class dominates often and minority can be ignored
        - Poor Recall for Minority Class
        - Works best for binary classification in comparison to multi-class labels

## Linear Models (KNN) 

- KNN
    - Assigns class labels based on the majority vote of nearest neighbors labels
    - Find the closest data points ; for e.g. Like asking nearby people for opinions.
    - Count how many belong to each category i.e. More votes = stronger choice.
    - Classify new data based on majority vote i.e. The new item gets labeled based on its nearest neighbors.
    - Preferred Linear model
    - Examples
       - Applicants similar in various features are categorized accordingly

## Tree-Based Model: Decision Tree

- A **simple Decision Tree** was used in EDA to explore how customer features like **income**, **loan status**, and **credit card use** relate to credit score categories. Helps validate patterns, guide **feature selection**, and spot early **class boundaries**.

### Insight
> Customers with **high income** and **no active loans** were mostly classified as **Good credit**.

### How it Informs Later Stages
- Features from the tree will be retained in final models.
- Tree-based classifiers (e.g., Decision Tree, Random Forest, XGBoost) will be trained and compared.
- We'll use **cross-validation** to evaluate performance and avoid overfitting.

```{r decisiontree}
# library(rpart)

# # Load a smaller or cleaner dataset
# df <- read.csv("score.csv")
# df$Credit_Score <- as.factor(df$Credit_Score)

# # Fit a basic decision tree
# tree_model <- rpart(Credit_Score ~ ., data = df, method = "class")

# # Plot the tree using base R
# plot(tree_model)
# text(tree_model)

```

## Tree-Based Model: Random Forest

- Builds many decision trees on random subsets of data and features and combines outputs via **majority vote** (classification).

### Strengths
- Reduces overfitting by averaging multiple trees, robust to noise and handles missing values, performs well on imbalanced data using class weights or sampling.

### Why We Used It
- Features like **Income**, **Age**, and **Credit History Length** showed non-linear relationships with `Credit_Score`, linear models may fail to capture non-linear trends in the data.
- Presence of **missing values** in fields like Salary and Credit_History.Tree-based models can handle **missing, skewed, and categorical data** more effectively and can naturally handle **feature interactions** and **class imbalance**.
- Detected **outliers** (e.g. Age = -500, Income = 0 or extremely high), strong **class imbalance** across `Credit_Score` levels (Standard ≫ Poor/Good). 

## Boosted Trees: XGBoost

- Trains models sequentially, where each tree **corrects errors** of the previous ones, combines many **weak learners** into a strong predictive model, highly efficient and scalable for large datasets.

### Why We Used It
- Multiple features showed **non-linear patterns** with `Credit_Score`(e.g., Age vs. Score, Income vs. Score showed curved/clustered trends).Dataset included **outliers and noise** (e.g., extreme age values like -500), Detected significant **class imbalance** — Standard ≫ Poor/Good, XGBoost excel at handling **non-linearity, outliers, and class imbalance**
- High **feature interactions** observed (e.g., Income + Delay count), XGBoost captures non-linear patterns and feature interactions like ‘High Income but frequent Delays’ that are difficult to model with linear approaches.

## Summary and Next Steps
- Found **missing values**, **outliers**, and skewed distributions in features like age, salary, and credit history
- Identified clear **class imbalance** in the target (`Credit_Score`), **PCA** suggested structure in the data with separable clusters across credit score classes, observed **non-linear relationships** and **feature interactions**  
  – e.g., income and delay count jointly affecting credit profiles.

### Modeling Plan Informed by EDA
- Linear models may be limited due to non-linearity and imbalance, tree-based models (e.g., Random Forest, XGBoost) are better suited based on data characteristics.Plan to apply **stratified cross-validation** to ensure robust model evaluation, will assess performance using F1 Score, ROC-AUC, and class-level recall.

## About Performance Metrics 

Performance Metrics defined and calculated here tells us the story on how best the models perform against train, test and validation data sets.
These metrics are evaluated across linear and tree-type models.

- Why Accuracy is not a reliable metric for this use-case ?

- Example scenario is 
      Misleading Accuracy → A high accuracy might not mean good classification (e.g., if 90% of data is class 0, predicting all as class 0 gives       90% accuracy, but fails completely)

## Performance Metrics (Simple) 

With Accuracy is ruled out, other metrics that helps to gauge the effectiveness of models are 

-  Sensitivity (Recall or True Positive Rate)
    - Higher sensitivity means fewer false negatives
    
-  Specificity (Precision or True Negative Rate)
    - Higher specificity means fewer false positives
    - Helpful metric for this credit classification use-case
    
-  F1-Score
    - Balances Precision & Recall, making it valuable for imbalanced data
    - High F1-score = Model is good at both minimizing false positives & false negatives

## Performance Metrics (Complex) 

To handle Class Imbalanced data sets, additional performance metrics (which are complex in calculation) are

- ROC (Receiver Operating Characteristic) - AUC (Area Under Curve)
    - Handles class imbalance well
    - AUC value ranges from 0 to 1 → Higher is better (closer to 1 = stronger classifier)
- PR  (Precision Recall) - AUC (Area Under Curve)
    - More reliable estimate for performance in highly imbalanced data 





 


